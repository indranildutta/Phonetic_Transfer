---
title: "Phonetic Tansfer analysis code"
author: "Auromita"
date: "1/23/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE)

options(scipen=999) # turn off scientific e notation, show full numbers. To turn on: options(scipen=0)
```

Load packages and data, create objects for lobanov normalized F1 and F2, create dataframe.

(note: file paths relative to project root folder (phonetic_transfer))

```{r, include=FALSE}

library(tidyverse)
library(phonR)
library(lme4)
#library(stargazer)
#library(modelsummary)
library(lmerTest)
#library(pwr)
#library(simr)
#library(EMAtools) # to calculate Cohen's d values from lmer objects

# package mixedpower (power analysis)
if (!require("devtools")) {
    install.packages("devtools", dependencies = TRUE)}

   devtools::install_github("DejanDraschkow/mixedpower") 
library(mixedpower)


# for replacing vowel symbols with unicode:
remapping_english <- c(`\\ae` = "æ", `\\vt` = "ʌ")
remapping_bengali <- c(A = "a:", E = "æ")


# read in data
df <- read.csv("RCode/formant_data_5.csv", check.names = TRUE) %>%
  rename_all(tolower) %>%
  mutate(vowel = remapping_english[as.character(vowel)]) %>%
  mutate_if(is.character,as.factor) 
  
str(df)

attach(df)

# pool measures from all 5 points in the vowel, then from that-- calculate the average F1 for each speaker, and normalize all data points from that speaker wrt that.  

normF1 <- with(df,normLobanov(cbind(f1_5,f1_15,f1_25,f1_35,f1_45)),group=subject)
normF2 <- with(df,normLobanov(cbind(f2_5,f2_15,f2_25,f2_35,f2_45)),group=subject)

df_lobanov <- data.frame(subject,gender,task,word,context,vowel,normF1,normF2)

detach(df)

rm(normF1,normF2)


# data frame with time as variable (long format)

# lobanov-normalized FFs
df_lobanov_time <- df_lobanov %>%
  rename(f1_1=f1_5, f1_2=f1_15, f1_3=f1_25, f1_4=f1_35, f1_5=f1_45, f2_1=f2_5, f2_2=f2_15, f2_3=f2_25, f2_4=f2_35, f2_5=f2_45) %>%
  # dplyr pivot() functions: convert data between long(indexed) and wide(cartesian) formats
  pivot_longer(starts_with("f"),
               names_to = c(".value", "time"),
               names_sep = "_",
               names_transform = list(time = as.factor)
               )
# Since there are multiple observations per row (F1 and F2) that need to go in separate columns in the output, the ".value" lets you specify that part of the variable name will be used to split the data in the output. The rest of the name becomes the variable "time"


# raw FFs with time as variable
df_time <- df %>%
  select(!c("filename", "f1_mid", "vowel_duration")) %>%
  rename(f1_1=f1_5, f1_2=f1_15, f1_3=f1_25, f1_4=f1_35, f1_5=f1_45, f2_1=f2_5, f2_2=f2_15, f2_3=f2_25, f2_4=f2_35, f2_5=f2_45) %>%
  # dplyr pivot() functions: convert data between long(indexed) and wide(cartesian) formats
  pivot_longer(starts_with("f"),
               names_to = c(".value", "time"),
               names_sep = "_",
               names_transform = list(time = as.factor)
               )

```


Log-mean normalization:

```{r, eval=FALSE, include=FALSE}

# data in long format for log-mean normalization 
df_long <- df %>%
  select(!c(vowel_duration, f1_mid)) %>%
  rename(f1_1=f1_5, f1_2=f1_15, f1_3=f1_25, f1_4=f1_35, f1_5=f1_45, f2_1=f2_5, f2_2=f2_15, f2_3=f2_25, f2_4=f2_35, f2_5=f2_45, iteration=filename) %>%
  pivot_longer(starts_with("f"), # pivot all the columns whose names start with "f"
               values_to = "ff_raw",
               # drop the f part, store the rest as values for new variables "formant" and "time"
               names_to = c(NA, "formant", "time"), 
               # how to divide up the variable name (f + formant + _ + time)
               names_pattern = "(.)(.)_(.)") %>% 
  # reorder columns
  relocate("subject", "gender", "task", "word", "context", "vowel", "time", "formant", "ff_raw") %>%
  mutate_if(is.character, as.factor) %>%
  # new column with log-transformed FFs
  mutate(ff_log = log(ff_raw)) %>%
  # composite variable vowel-formant (estimate for the dialectal reference pattern-- the target ff for that vowel in the dialect)
  mutate (vowel.formant = factor(interaction(vowel, formant)))


# fitting a regression model to predict log-transformed formant-frequencies (ff-log) for a given vowel, formant and speaker as the sum of a speaker displacement term (subject) and normalized vowel-formant effects (vowel.formant)  (Barreda&Neary 2018)

### only vowel midpoints for figures
df_long <- df_long %>%
  filter(time==5)

attach(df_long)

# forcing the intercept to 0 so that coefficients are calculated for all levels of "subject"
M = lm (ff_log ~ 0 + subject + vowel.formant, contrasts = list(vowel.formant = contr.sum))
summary(M)

# coefficients of the speaker effect (estimate of speaker parameter)
speaker_coeff = dummy.coef(M)$subject
# coefficients of the vowel.formant effect (estimate of dialect reference/target formant for each vowel)
vowel_coeff = dummy.coef(M)$vowel.formant

detach(df_long)

# from each log-transformed ff of a particular speaker, want to subtract the speaker coefficient 

temp <- as.data.frame(speaker_coeff) %>%
  rownames_to_column(var = "subject")

df_long <- df_long %>%
  full_join(temp, by = "subject") 

df_long <- df_long %>%
  mutate(ff_log_normalized = ff_log - speaker_coeff) 

df_long <- df_long %>%
  mutate(ff_exp_normalized = exp(ff_log_normalized))

rm(temp,speaker_coeff,vowel_coeff)


# putting F1 and F2 in separate columns for plotting (pivot_wider doesn't work because there are no unique identifiers for each vowel, since there were multiple reps in each condition). So crude workaround:

df_clean <- df_long %>%
  select(subject, gender, task, iteration, word, context, vowel, time, formant, ff_log_normalized, ff_exp_normalized, ff_raw) 

f1 <- df_clean %>%
  filter(formant == 1)

f2 <- df_clean %>%
  filter(formant == 2)

df_normalized <- bind_cols(f1,f2$ff_log_normalized,f2$ff_exp_normalized, f2$ff_raw)%>%
  mutate(formant=NULL) %>%
  rename(f1_log = ff_log_normalized, f1_exp = ff_exp_normalized, f1_raw = ff_raw, f2_log =...13, f2_exp = ...14, f2_raw = ...15)


rm(df_clean, f1, f2)

```


Baseline Bengali data from the Shruti corpus:

```{r, include=FALSE}

# list of files in directory (paths relative to project root)
file_list <- list.files(path="shruti/merged_speaker_data", full.names = T)
vowel_list <- "oh|u|i|ee|A|E|aa"

# create dataframe by reading data from each file in folder
df_ben <- file_list %>%
  set_names(.) %>%
  # apply read_table() to each file, read only specified columns
  map_df(read_table, .id = "FileName", col_types = cols_only(rowLabel ='f', Time ='d', F1 ='d', F2 ='d')) %>%
  # rename columns
  rename(speaker=FileName, vowel=rowLabel, time_absolute=Time, f1=F1, f2=F2) %>%
  # only keep rows that have vowels
  filter(grepl(vowel_list, vowel)) %>%
  # clean column name (remove file path)
  mutate(speaker = str_replace(speaker, "shruti/merged_speaker_data/", "")) %>%
  mutate(speaker = str_replace(speaker, "_merged.txt", "")) %>%
  mutate(speaker = as.factor(speaker)) %>%
  droplevels() 
  

# columns for gender and time point (1-20)

male = c("abd", "chandan", "kunal", "plb", "sayan", "sudha", "swarnendu", "amitava", "deb", "mainak", "rajashree", "sm", "suman", "xxx", "bd", "jit", "moy", "samaresh", "smt", "suparna", "chan", "jyotirmoy", "padma", "sandi", "styabrata", "suparnakdas")

female = c("manika", "msm", "punam", "rita",  "ritwika",  "shyamoshree",  "suranjana",  "tml")

timepoints <- c(1:20)


# add column for gender using mutate() and ifelse()
# add column for time (iterate over 1:20; length.out: desired length of final vector (here- total no. of rows))
df_ben <- df_ben %>%
  mutate(gender = ifelse(speaker %in% male, "male", "female"),
         time = rep(timepoints, length.out = 2458340), .after = time_absolute,
         time = as.factor(time)
         ) 


# Lobanov normalization (normalizing over all vowels here. For the English data-- only have 2 vowels.)
# attach(bengali_df)
# lobanov <- with(bengali_df,normLobanov(cbind(f1,f2), group = speaker))
# bengali_lobanov <- data.frame(speaker, gender, vowel, time, lobanov)
# rm(lobanov)
# detach(bengali_df)

rm(female, file_list, male, timepoints, vowel_list) 


# subset of dataframe with only vowels of interest (A and E) at vowel midpoints (timepoints 8-12),
# removing outlier formant values (<300 Hz): ~ 11% of rows. In full dataset (all vowels): ~14%

df_ben_AE <- df_ben %>%
  filter(grepl("A|E", vowel), time %in% (8:12), f1>600) %>%
  droplevels() %>%
  mutate(time_absolute = NULL) %>%
  relocate(gender, .after = speaker) %>%
  mutate (vowel = remapping_bengali[as.character(vowel)])
  


```

Log-mean normalization for Bengali data:

```{r, eval=FALSE, include=FALSE}

# subset of dataframe with only vowels of interest (A and E) at vowel midpoints (timepoints 8-12),
# removing outlier formant values (<300 Hz): ~ 11% of rows. In full dataset (all vowels): ~14%

df_ben_AE <- df_ben %>%
  filter(grepl("A|E", vowel), time %in% (8:12), f1>600) %>%
  droplevels() %>%
  mutate(time_absolute = NULL) %>%
  relocate(gender, .after = speaker)


# data in long format for log-mean normalization 

df_ben_long <- df_ben_AE %>%
  pivot_longer(starts_with("f"), # pivot all the columns whose names start with "f"
               values_to = "ff_raw",
               # drop the f part, store the rest as value for new variable "formant" 
               names_to = c(NA, "formant"), 
               # how to divide up the variable name (f + formant)
               names_pattern = "(.)(.)") %>% 
  mutate_if(is.character, as.factor) %>%
  # new column with log-transformed FFs
  mutate(ff_log = log(ff_raw)) %>%
  # composite variable vowel-formant (estimate for the dialectal reference pattern-- the target ff for that vowel in the dialect)
  mutate (vowel.formant = factor(interaction(vowel, formant)))



# fitting a regression model to predict log-transformed formant-frequencies (ff-log) for a given vowel, formant and speaker as the sum of a speaker displacement term (subject) and normalized vowel-formant effects (vowel.formant)  (Barreda&Neary 2018)

attach(df_ben_long)

# forcing the intercept to 0 so that coefficients are calculated for all levels of "subject"
M_ben = lm (ff_log ~ 0 + speaker + vowel.formant, contrasts = list(vowel.formant = contr.sum))
summary(M)

# coefficients of the speaker effect (estimate of speaker parameter)
speaker_coeff = dummy.coef(M_ben)$speaker
# coefficients of the vowel.formant effect (estimate of dialect reference/target formant for each vowel)
vowel_coeff = dummy.coef(M_ben)$vowel.formant

detach(df_ben_long)

# from each log-transformed ff of a particular speaker, want to subtract the speaker coefficient 

temp <- as.data.frame(speaker_coeff) %>%
  rownames_to_column(var = "speaker")

df_ben_long <- df_ben_long %>%
  full_join(temp, by = "speaker") 

df_ben_long <- df_ben_long %>%
  mutate(ff_log_normalized = ff_log - speaker_coeff) 

df_ben_long <- df_ben_long %>%
  mutate(ff_exp_normalized = exp(ff_log_normalized)) 


rm(temp,speaker_coeff,vowel_coeff)


# putting F1 and F2 in separate columns for plotting (pivot_wider doesn't work because there are no unique identifiers for each vowel, since there were multiple reps in each condition). So crude workaround:

df_clean <- df_ben_long %>%
  select(speaker, gender, vowel, time, formant, ff_log_normalized, ff_exp_normalized, ff_raw) 

f1 <- df_clean %>%
  filter(formant == 1)

f2 <- df_clean %>%
  filter(formant == 2)

df_ben_normalized <- bind_cols(f1, f2$ff_log_normalized, f2$ff_exp_normalized, f2$ff_raw) %>%
  mutate(formant=NULL) %>%
  rename(f1_log=ff_log_normalized, f1_exp=ff_exp_normalized, f1_raw=ff_raw, f2_log=...9, f2_exp=...10, f2_raw=...11)


rm(df_clean, f1, f2)

```


Plotting vowel data:


```{r}

# add column with unicode vowels (if needed)
#df_normalized$unicodevowel <- remapping_english[as.character(df_normalized$vowel)]

# plot: unilingual English, mixed, and baseline Bengali
xlim <- c(2500,600)
ylim <- c(1100,500)



## plot for baseline Bengali vs unilingual English
df_eng_unilingual <- df_time %>%
  filter(time %in% (3:5), context == "e")

with(df_ben_AE,plotVowels(f1, f2, vowel,
                        plot.tokens = F, plot.means = TRUE, pch.tokens = vowel, pch.means = vowel, 
                        var.col.by = NULL, var.sty.by = NULL, 
                        cex.tokens = 0.8, cex.means = 1, alpha.means = 0.8,alpha.tokens = 0.3,
                        ellipse.line = F, ellipse.fill = T, fill.opacity = 0.2, 
                        poly.line = F, poly.order = c("a:","æ"),
                        xlim = xlim, ylim = ylim, 
                        #legend.kwd="bottomleft", legend.args = list(legend=c("baseline æ","baseline a:")), 
                        pretty = TRUE))
par(new=TRUE)

# English vowels
with(df_eng_unilingual, plotVowels(f1, f2, vowel, 
                               plot.tokens = F, plot.means = TRUE , pch.tokens = vowel, pch.means = vowel,
                               var.col.by = NULL, var.sty.by = NULL, 
                               cex.tokens = 0.8, cex.means = 1, alpha.means = 0.5,alpha.tokens = 0.3,
                               ellipse.line = T, ellipse.fill = F, fill.opacity = 0.1,
                               poly.line = F, poly.order = c("æ","ʌ"),
                               xlim = xlim, ylim = ylim, 
                               #legend.kwd = "bottomright", 
                               axes=FALSE, pretty = TRUE))

rm(df_eng_unilingual)

```


```{r}
## final result plot:

df_eng_plots <- df_time %>%
  filter(time %in% (3:5))

# Bengali vowels
with(df_ben_AE,plotVowels(f1, f2, vowel,
                        plot.tokens = F, plot.means = TRUE, pch.tokens = vowel, pch.means = vowel, 
                        var.col.by = vowel, var.sty.by = vowel, 
                        cex.tokens = 0.8, cex.means = 1, alpha.means = 0.8,alpha.tokens = 0.3,
                        ellipse.line = F, ellipse.fill = T, fill.opacity = 0.2,
                        poly.line = F, poly.order = c("a:","æ"),
                        xlim = xlim, ylim = ylim, 
                        legend.kwd="bottomleft", legend.args = list(legend=c("baseline æ","baseline a:")), 
                        pretty = TRUE))
par(new=TRUE)

# English vowels
with(df_eng_plots, plotVowels(f1, f2, vowel, group = context,
                               plot.tokens = F, plot.means = TRUE , pch.tokens = vowel, pch.means = vowel,
                               var.col.by = NULL, var.sty.by = context, 
                               cex.tokens = 0.8, cex.means = 1, alpha.means = 0.5,alpha.tokens = 0.3,
                               ellipse.line = T, ellipse.fill = T, fill.opacity = 0.1,
                               poly.line = F, poly.order = c("æ","ʌ"),
                               xlim = xlim, ylim = ylim, 
                               legend.kwd = "bottomright", legend.args = list(legend=c("unilingual","mixed")),
                               axes=FALSE, pretty = TRUE))


rm(df_eng_plots)


```

## Gender differences in IE vowels:
```{r}

# Lobanov-normalized FFs-- before gender normalization
xlim = c(3, -3)
ylim = c(4, -4)

with(df_lobanov_time, plotVowels(f1, f2, vowel, group = gender,
                               plot.tokens = F, plot.means = TRUE , pch.tokens = vowel, pch.means = vowel,
                               var.col.by = vowel, var.sty.by = gender, 
                               cex.tokens = 0.8, cex.means = 1, alpha.means = 0.5,alpha.tokens = 0.3,
                               ellipse.line = T,  ellipse.fill = T,
                               poly.line = F, poly.order = c("\\ae","\\vt"),
                               xlim = xlim, ylim = ylim, 
                               legend.kwd = "bottomright", legend.args = gender,
                               fill.opacity = 0.05, pretty = TRUE))


```


Gender difference is not removed after lobanov normalization. Normalizing by gender:

```{r}

gender_normalized <- with(df_lobanov_time, normLobanov(cbind(f1,f2), group=gender))

df_lobanov_gender_time <- df_lobanov_time %>%
  select(!c(f1,f2)) %>%
  mutate(f1 = gender_normalized$f1,
         f2 = gender_normalized$f2)

rm(gender_normalized)  
```

Plots after gender normalization:

```{r}

# raw FFs
with(df_lobanov_gender_time, plotVowels(f1, f2, vowel, group = gender,
                               plot.tokens = F, plot.means = TRUE , pch.tokens = vowel, pch.means = vowel,
                               var.col.by = vowel, var.sty.by = gender, 
                               cex.tokens = 0.8, cex.means = 1, alpha.means = 0.5,alpha.tokens = 0.3,
                               ellipse.line = T,  ellipse.fill = T,
                               poly.line = F, poly.order = c("\\ae","\\vt"),
                               xlim = xlim, ylim = ylim, 
                               legend.kwd = "bottomright", legend.args = gender,
                               fill.opacity = 0.05, pretty = TRUE))


```

## LME models:

writeup-- (using data from all 5 points. This should be okay because the pre-vocalic consonants are all the same, so any effect of the consonant should be uniform. A difference between contexts in the early part of the vowel suggests differences in target FFs)

Gender effects: try:
- normalize by gender (done)
- use log-mean normalized values

Hypotheses:

- FFs are different across vowels
- FFS are different across contexts
- the two vowel categories are differently affected (precise direction/extent of shift in FFs depends on category, position in vowel space, presence/absence of contrast in L1)
So main effect of interest: context*vowel

- if transfer patterns differ across tasks, then expect: context\*task interaction, OR context\*vowel\*task interaction

- FFs should change across time, so effect of time on FF. But what time is doing to the FF (direction of change)-- depends on the vowel category. So expect: time\*vowel interaction

- if transfer patterns change across time, then expect: context\*time interaction, OR context\*vowel\*time interaction

Random effects structure:

- subject: intercept, context, vowel (target FF different across speakers), context\*vowel interaction (transfer pattern different across speakers)

-item: intercept (FF for vowel different in each item), context (the effect of context is different for each word), NOT vowel (between-item variable), NOT gender (even though within-item, no reason to believe that effect of gender on FF is different for each item)


```{r, eval=FALSE}

temp <- df_lobanov_gender_time %>%
  mutate(time = as.numeric(time)) %>%
  mutate(time = scale(time))

# F1

# full model
phon_acco.model_f1 = lmer(f1 ~ 
                         time*vowel + vowel*context*task + time*task +
                         (1+context|word) + (1+context+vowel|subject), 
                         data= temp, REML=FALSE, control = lmerControl(optimizer = "bobyqa"))

summary(phon_acco.model_f1)
summary(rePCA(phon_acco.model_f1))

# Cohen's d values (from EMAtools package):
d_F1 <- lme.dscore(phon_acco.model, data=temp, type='lme4')


##--> find way to export fixed effects table (estimate, SE, p values) to LaTex

#stargazer(phon_acco.model_f1)

# null model
null.model = lmer(f1~
                   time*vowel + vowel*context*task + time*task +
                   (1+context|word) + (1+context+vowel|subject), 
                   data= temp, REML=FALSE, control = lmerControl(optimizer = "bobyqa")) 

summary(null.model)
summary(rePCA(null.model))

anova(phon_acco.model_f1, null.model)

# some weird interaction with lmerTest package. Detach that first
#stargazer(phon_acco.model)
#modelsummary(phon_acco.model)

```

Random effects structure: (1+context|word) + (1+context+vowel|subject). But by-word random slope for context is explaining barely any variance, and null model is not converging. So removing that: (1|word) + (1+context+vowel|subject)

- Full model: time\*vowel + vowel\*context
- null model: time*vowel
full model is better
- null model: time*vowel + context
full model still better

Shows: F1 differs across contexts (mediated by vowel category). So transfer is happening.

Now want to see: effect of task?
If there is a difference in transfer patterns across tasks, then expect task to interact with the vowel\*context variable 
- full model: gender + time\*vowel + vowel\*context\*task 
-task by itself affects F1 (unexpected)
- marginal effect of context\*task (effect of context on F1 is moderated by task) -- expected
- significant effect of vowel\*context\*task (transfer pattern is moderated by task) -- expected

- null model: gender + time\*vowel + vowel\*context
full model better

- null model: gender + time\*vowel + vowel\*context + task
(because adding task is significant on its own, and adding it as an interaction term increases model parameters, want to see if having it as an interaction significantly improves fit)
full model better -- task does interact with context and vowel

- null model: gender + time\*vowel + vowel\*context + task\*context 
to see if three-way interaction is needed. Is task only interacting with context?
nope, context\*task not significant here
full model better

- null model: gender + time\*vowel + vowel\*context + task\*vowel 
to see if three-way interaction is needed. Is task only interacting with vowel?
task\*vowel is significant
but anova shows that the full model is still better

So: need the task\*context\*vowel interaction

vowel\\vt:contexte:tasks      -1.185e-01  3.551e-02  1.162e+04  -3.338 0.000848 ***
vowelvt:contexte has a negative slope (for vt, in the English context, F1 is lower--vowel is higher, i.e. vowel is lower in b context. This is the main transfer effect.)
Now, the interaction of this with tasks has a negative slope (this entire thing is increased (becomes more in magnitude) in the s task compared to the p task)
i.e., more transfer in the s task?

Want to see: is the effect of task local (decreases with time)? Since the effect of just task on F1 is unexpected, want to see if it might be an artifact of the paradigm/stimuli
tasks:time     -7.506e-02  1.751e-02  1.162e+04  -4.286 1.83e-05 ***
(tasks has a positive slope (F1 increases/vowel lowers in s task);
time has a positive slope (F1 increases/vowel lowers with time);
interaction term has a negative slope: the effect of tasks (lowering of vowel) reduces with increase in time: effect of task is more in the beginning: local effect)


Is transfer pattern changing with time? 
vowel\*context\*time: not significant
context\*time: not significant

full model: gender + vowel\*context\*task\*time
null model: gender + time\*vowel + vowel\*context\*task + task\*time -- final model
null model better (time doesn't need to interact with context, or with vowel\*context)-- transfer pattern is not changing over time-- the targets are different
No effect of context on formant dynamics. There is a linear scaling of the trajectory--targets are different.

Do transfer patterns differ across gender?
full model: time\*vowel + vowel\*context\*task\*gender + task\*time
no effect of vowel\*context\*gender-- no difference in transfer patterns
but: task\*gender is significant: men show more lowering in the sentence task than females --unexpected

```{r,eval=FALSE}
# F2

# full model
phon_acco.model_f2 = lmer(f2 ~ 
                         vowel*time + vowel*context*task + task*time +
                         (1+context|word) + (1+context+vowel|subject), 
                         data= temp, REML=FALSE, control = lmerControl(optimizer = "bobyqa"))
summary(phon_acco.model_f2)
summary(rePCA(phon_acco.model_f2))

# Cohen's d values (from EMAtools package):
d_F2 <- lme.dscore(phon_acco.model, data=temp, type='lme4')

##--> find way to export fixed effects table (estimate, SE, p values) to LaTex



# null model
null.model = lmer(f2~
                   time*vowel + vowel*context*task*time +
                   (1+context|word) + (1+context+vowel|subject), 
                    data= temp, REML=FALSE, control = lmerControl(optimizer = "bobyqa"))

summary(null.model)

anova(phon_acco.model_f2, null.model)


```

- Full model: time\*vowel + vowel\*context
effect of vowel: vt is more backed than ae
effect of time: F2 changes with time, but direction is moderated by vowel: ae is fronted, vt less so
marginal effect of context: both vowels are fronted in b context
main effect of vowel*context: ae is fronted in the b context, vt is very very slightly backed
- null model: time*vowel
full model is better
- null model: time*vowel + context
full model still better

Shows: F2 differs across contexts (mediated by vowel category). So transfer is happening in front-back dimension.

Now want to see: effect of task?
If there is a difference in transfer patterns across tasks, then expect task to interact with the vowel\*context variable 
- full model: time\*vowel + vowel\*context\*task 
fails to converge.
fitting 0-correlation model (remove correlations from by-subject random effects): still doesn't converge. VCV matrix shows by-subject slopes for vowel have correlation of 1. So removing that. Converges now.
independent effect of task: vowels are backed in the s task

-task by itself affects F2 (unexpected)
- marginal effect of context\*task (effect of context on F2 is moderated by task) -- more fronting (in b context) during s task
- effect of vowel\*context\*task: not significant 

- null model: gender + time\*vowel + vowel\*context
full model better
---
- null model: gender + time\*vowel + vowel\*context + task
(because adding task is significant on its own, and adding it as an interaction term increases model parameters, want to see if having it as an interaction significantly improves fit)
full model better -- task does interact with context and vowel

- null model: gender + time\*vowel + vowel\*context + task\*context 
to see if three-way interaction is needed. Is task only interacting with context?
nope, context\*task not significant here
full model better

- null model: gender + time\*vowel + vowel\*context + task\*vowel 
to see if three-way interaction is needed. Is task only interacting with vowel?
task\*vowel is significant
but anova shows that the full model is still better

So: need the task\*context\*vowel interaction

vowel\\vt:contexte:tasks      -1.185e-01  3.551e-02  1.162e+04  -3.338 0.000848 ***
vowelvt:contexte has a negative slope (for vt, in the English context, F1 is lower--vowel is higher, i.e. vowel is lower in b context. This is the main transfer effect.)
Now, the interaction of this with tasks has a negative slope (this entire thing is increased (becomes more in magnitude) in the s task compared to the p task)
i.e., more transfer in the s task?

Want to see: is the effect of task local (decreases with time)? Since the effect of just task on F1 is unexpected, want to see if it might be an artifact of the paradigm/stimuli
tasks:time     -7.506e-02  1.751e-02  1.162e+04  -4.286 1.83e-05 ***
(tasks has a positive slope (F1 increases/vowel lowers in s task);
time has a positive slope (F1 increases/vowel lowers with time);
interaction term has a negative slope: the effect of tasks (lowering of vowel) reduces with increase in time: effect of task is more in the beginning: local effect)


Is transfer pattern changing with time? 
vowel\*context\*time: not significant
context\*time: not significant

full model: gender + vowel\*context\*task\*time
null model: gender + time\*vowel + vowel\*context\*task + task\*time -- final model
null model better (time doesn't need to interact with context, or with vowel\*context)-- transfer pattern is not changing over time-- the targets are different
No effect of context on formant dynamics. There is a linear scaling of the trajectory--targets are different.

Do transfer patterns differ across gender?
full model: time\*vowel + vowel\*context\*task\*gender + task\*time
no effect of vowel\*context\*gender-- no difference in transfer patterns
but: task\*gender is significant: men show more lowering in the sentence task than females --unexpected


Power analysis:

```{r, include=FALSE, eval=FALSE}

# F1:

fixef(phon_acco.model_f1)


# using the mixedpower package

# dummy numeric variable for subject:
remapping_subjects <- c("f1"="01", "f2"="02", "f3"="03", "f4"="04", "f5"="05", "m1"="06", "m2"="07", "m3"="08", "m4"="09", "m5"="10")

temp <- temp %>%
  mutate(subject_num = remapping_subjects[as.character(subject)]) %>%
  mutate(subject_num = as.numeric(subject_num))

power_trial <- mixedpower(model = phon_acco.model_f1, data = temp,
                          fixed_effects = c("vowel","context","time","task"),
                          simvar = "subject_num", steps = c(5,10,15,20),
                          critical_value = 2)

power_plot <- multiplotPower(power_trial, filename = "powerplot_f1_databased.png")

# next, ran a power analysis assuming smaller effect sizes by reducing the beta-coefficients by 15\%. (Use the model summary, make a vector of all beta-values, including intercept, add as argument of SESOI= )

beta.f1 <-fixef(phon_acco.model_f1) # unname() #creates a named vector. To remove names, function: unname(beta.f1)


beta.f1

# setting beta-values to 85% of original
beta.f1<- beta.f1 %>%
  map(~ . * 1.05) %>% # map each element of the vector to the result of applying the function f to it (creates a list)
  unlist()  # convert back to a named vector

beta.f1

power_f1_plus5 <- mixedpower(model = phon_acco.model_f1, data = temp,
                          fixed_effects = c("vowel","context","time","task"),
                          simvar = "subject_num", steps = c(5,8,10,12,15), n_sim = 500,
                          SESOI = beta.f1, databased = F,
                          critical_value = 2)


powerplot_f1_plus5 <- multiplotPower(power_f1_plus5, filename = "powerplot_f1_plus5.png")

```

done: F1: -5, -15, +15
F2: -15, +15

```{r, eval=FALSE, include=FALSE}

# F2:


# using the mixedpower package

# dummy numeric variable for subject:
remapping_subjects <- c("f1"="01", "f2"="02", "f3"="03", "f4"="04", "f5"="05", "m1"="06", "m2"="07", "m3"="08", "m4"="09", "m5"="10")

temp <- temp %>%
  mutate(subject_num = remapping_subjects[as.character(subject)]) %>%
  mutate(subject_num = as.numeric(subject_num))

power_f2_databased <- mixedpower(model = phon_acco.model_f2, data = temp,
                          fixed_effects = c("vowel","context","time","task"),
                          simvar = "subject_num", steps = c(5,10,15,20),
                          critical_value = 2)

powerplot_f2_databased <- multiplotPower(power_f2_databased, filename = "powerplot_f2_databased.png")

# next, ran a power analysis assuming smaller effect sizes by reducing the beta-coefficients by 15\%. (Use the model summary, make a vector of all beta-values, including intercept, add as argument of SESOI= )

beta.f2 <-fixef(phon_acco.model_f2) #creates a named vector. To remove names, function: unname(beta.f1)

beta.f2

# setting beta-values to 85% of original
beta.f2 <- beta.f2 %>%
  map(~ . * 1.05) %>% # map each element of the vector to the result of applying the function f to it (creates a list)
  unlist()  # convert back to a named vector

beta.f2

power_f2_plus5 <- mixedpower(model = phon_acco.model_f2, data = temp,
                          fixed_effects = c("vowel","context","time","task"),
                          simvar = "subject_num", steps = c(5,8,10,12,15), n_sim = 500,
                          SESOI = beta.f2, databased = F,
                          critical_value = 2)

 powerplot_f2_plus5 <- multiplotPower(power_f2_plus5, filename = "powerplot_f2_plus5.png")

```

Reporting power analyses:

Now that you have conducted your power analysis you will need to report it in APA style. This sentence (or two) is usually placed in the Results section of a research report near the start of the results when the assumptions and adequacy of the sample size are being reported.The sentence that you write should include the following:

- The type of statistical test that you will be using to analyse the data (e.g., an independentsample t-test)
- The type of power analysis that you did–either a priori or post hoc (generally researchersreport the a priori power analysis)
- The name of the software program that you used to conduct the power analysis and a citation to the authors and date of publication (see the G\*Power website listed above forthe journal article to cite)
- The level of power aspired to for a prior analyses, or the actual power achieved for post hocanalysesThe estimated or actual effect size used in the calculationThe alpha level used in the calculation
- For an a priori power analysis, report the sample size needed to achieve the minimumacceptable power level.For post hoc analysis, report achieved sample size.

All this detail needs to be reported concisely and in one or two sentences at most.I’ve written an example reportfor question 1 for a power of .80 below:

An a priori power analysis was conducted using G*Power3 (Faul, Erdfelder, Lang, & Buchner,2007) to test the difference between two independent group means using a two-tailed test,a medium effect size (d= .50), and an alpha of .05.Result showed that a total sample of 128 participants with two equal sized groups ofn= 64 was required to achieve a power of .80.

-------

A statistical power analysis was performed for sample size estimation, based on data from pilot
study/published study X (N=…), comparing …. to ….. The effect size (ES) in this study was ….,
considered to be extremely large/large/medium/small using Cohen's (1988) criteria. With an
alpha = .05 and power = 0.80, the projected sample size needed with this effect size (GPower
3.1 or other software) is approximately N = ….. for this simplest between/within group
comparison. Thus, our proposed sample size of ..N+.. will be more than adequate for the main
objective of this study and should also allow for expected attrition and our additional objectives
of controlling for possible mediating/moderating factors/subgroup analysis, etc.

------
There is literature saying that power analysis is not very relevant if an effect has already been observed (cite), and should not change the interpretation of already observed significant effects. Here, did a retrospective power analysis to guide how null results should be interpreted. 
say:power for vowel\*context is lower than 80%. Means that: for a effect of this size, if it is present, there is a 40% chance that we may not observe it in the data. This means that if we get a null result, we should be careful about interpreting this as an absence of effect, and future studies should instead repeat this with larger sample sizes. For vowel\*context\*task, however, power is more than 80%. Thus, we can reasonably interpret the absence of statistical significance as an absence of the effect. 

```{r, eval=FALSE, include=FALSE}
# scratch pad: additional code for manuscript revision

temp <- df_lobanov %>%
  filter(vowel == "æ")

# total: 2333; ʌ: 1137, æ: 1196

```